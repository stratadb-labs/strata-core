# v0.12: Intelligent Search

**Theme**: `search()` gets smarter. No new API — the existing search transparently improves when a model is configured.

This is the first LLM integration. An external model (Qwen3 or similar) expands natural language queries into keyword reformulations, semantic rephrasings, and hypothetical documents. The existing hybrid search engine (v0.8) runs each variant across all primitives. Results are fused with weighted Reciprocal Rank Fusion. No re-ranking, no synthesis — the value is in query expansion, not answer generation.

---

## Query Expansion

User asks a natural language question. The model expands it into typed search variants. Each variant runs through the existing cross-primitive HybridSearch pipeline.

```
"what tools did the agent use recently?"
        |
        v
   Model: query expansion
   -> lex: agent tools usage
   -> lex: tool_call tool_use
   -> vec: which tools were used by the agent in recent operations
   -> hyde: The agent invoked several tools including search, code generation,
            and file operations during the most recent session.
        |
        v
   Multi-pass HybridSearch (BM25 + vector per pass)
        |
        v
   Weighted RRF fusion (original query 2x, expansions 1x)
        |
        v
   Ranked results (same format as existing search)
```

### Expansion types

| Type | Purpose | Search mode |
|------|---------|-------------|
| `lex` | Keyword reformulations, technical terms | BM25 only |
| `vec` | Natural language semantic rephrasings | BM25 + vector (hybrid) |
| `hyde` | Hypothetical document text (HyDE) | Vector only (embed the hypothesis) |

### What the model does in this release

- Generates keyword reformulations (`lex:`) for BM25 matching
- Generates semantic rephrasings (`vec:`) for vector similarity
- Generates hypothetical document passages (`hyde:`) for HyDE-style retrieval

### What the model does NOT do yet

- No result re-ranking (results scored by existing BM25/vector/RRF pipeline)
- No answer synthesis (no generated text in the response)
- No multi-step retrieval (no "search, then search based on those results")
- No temporal expression parsing (no "last hour" -> time range mapping)

---

## Model Configuration

Users bring their own inference endpoint. `configure_model()` points Strata at any OpenAI-compatible API — Ollama, vLLM, llama.cpp server, OpenAI, or any compatible provider.

```
db.configure_model(
    endpoint: "http://localhost:11434/v1",
    model: "qwen3:1.7b",
)
```

- **No model configured**: Search works exactly as before. Zero change.
- **Model configured**: Search transparently runs query expansion when it helps.
- **Model endpoint down**: Search falls back to existing behavior. Never hard-fails.

### Strong signal detection

Not every query needs LLM expansion. When BM25 already has a confident match, the LLM is skipped entirely:

1. Run a quick BM25 probe (~10ms)
2. Top result score >= 0.85 AND gap to #2 >= 0.15?
3. YES -> return immediately, skip model call
4. NO -> expand query, run multi-pass search

This keeps simple keyword searches fast while improving natural language queries.

---

## Memory footprint

| Component | RAM |
|-----------|-----|
| MiniLM-L6-v2 (from v0.7) | ~80 MB |
| Query expansion model | 0 MB (external API) |
| **Total** | **~80 MB** |

No additional memory. The model runs externally — Strata makes HTTP calls to the configured endpoint.

---

## Graceful degradation

Every failure mode degrades to existing search quality:

| Failure | Behavior |
|---------|----------|
| No model configured | Existing search (BM25 + vector + RRF) |
| Model endpoint unreachable | Existing search |
| Model returns garbage | Existing search (parser falls back) |
| Model times out | Existing search |
| BM25 already confident | Existing search (LLM skipped) |

Search never hard-fails because of the model. The LLM is a best-effort accelerator.

---

## Dependencies

- v0.8 (hybrid search — the model generates queries that the hybrid pipeline executes)
- `ureq` (already a workspace dep — for HTTP calls to model endpoint)

## Open questions

- How much context to feed the model — just the query string, or DB schema info (available event types, collection names)?
- Should expansion results be cached per query to avoid repeated model calls?
- What's the right strong signal threshold? 0.85 with 0.15 gap is borrowed from QMD — may need tuning.
- Should we support non-OpenAI-compatible APIs (e.g., raw HTTP with custom format)?
