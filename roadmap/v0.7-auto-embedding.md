# v0.7: Auto-Embedding

**Theme**: Insert data, it's searchable. Zero configuration.

MiniLM-L6-v2 runs inside `strata-intelligence`, automatically generating 384-dimensional embeddings for text inserted into any primitive. Shadow vector collections make everything semantically searchable without the user managing an embedding pipeline.

---

## Auto-Embedding Pipeline

When text is inserted into any text-bearing primitive, the intelligence layer generates an embedding and populates a shadow vector collection.

```
db.kv_put("doc:123", "The agent completed the search task")
        │
        ▼ (if auto_embed enabled)
   MiniLM-L6-v2 → 384-dim embedding
        │
        ▼
   _system_kv collection (shadow vector, branch-scoped)
```

- **Model**: all-MiniLM-L6-v2, ~22M parameters, ~80MB RAM
- **Shadow collections**: `_system_kv`, `_system_json`, `_system_event` — one per text-bearing primitive
- **Branch-scoped**: shadow collections follow branch isolation, lifecycle-managed by intelligence layer
- **Opt-in**: `Strata::builder().auto_embed(true).open()`
- **Zero overhead when disabled**: database behaves exactly as today

### Configuration

```rust
let db = Strata::builder()
    .path("/data")
    .auto_embed(true)
    .embed_primitives(&["kv", "json", "event"])
    .open()?;
```

### Sync vs. async

| Approach | Write latency | Search consistency |
|----------|--------------|-------------------|
| Synchronous | +~1ms per insert | Immediately searchable |
| Asynchronous | Unchanged | Eventually consistent (ms-scale lag) |

---

## Nano Inference Runtime (MiniLM only)

Minimal runtime for the embedding model. Not a general-purpose inference server.

- **Continuous batching**: Batch embedding requests (bulk insert) through MiniLM in one forward pass
- Models downloaded on first use to `~/.stratadb/models/`, checksum-verified
- If model unavailable, auto-embed is disabled — database still works

### Runtime options

| Option | Pros | Cons |
|--------|------|------|
| **candle** (Rust-native) | Pure Rust, compiles with cargo | Younger ecosystem |
| **ONNX Runtime** (FFI) | Mature MiniLM support, hardware abstraction | Large dependency |

### Feature flag

```toml
[features]
intelligence-embed = []    # MiniLM auto-embedding (~80MB RAM)
```

---

## Dependencies

- `_system_*` vector namespace (already shipped)
- v0.5 branch foundations (auto-embeddings must respect branch isolation)
- v0.6 SDKs (Python/Node users get auto-embedding when this ships)

## Open questions

- Sync vs. async embedding — which is the right default?
- Model update path when better small embedding models ship
