# v0.9: Natural Language Search (Basic)

**Theme**: Ask a question, get results. Qwen3 translates natural language into typed sub-queries.

This is the first LLM integration. Qwen3-1.3B runs inside `strata-intelligence` and decomposes natural language queries into multi-primitive search operations. Results come back raw — no re-ranking, no synthesis. The value is in query understanding, not answer generation.

---

## Query Decomposition

User asks a natural language question. Qwen3 translates it into typed sub-queries across primitives. The existing hybrid search engine (v0.8) executes them.

```
"what tools did the agent use in the last hour?"
        │
        ▼
   Qwen3: query decomposition
   → event search: type=tool_call, time > 1h ago
   → KV search: prefix "agent:tool:"
   → vector search: semantic similarity to "tools agent used"
        │
        ▼
   Enhanced hybrid search (v0.8)
        │
        ▼
   Raw results (no re-ranking, no synthesis)
```

### What Qwen3 does in this release

- Parses natural language into typed sub-queries across primitives
- Maps temporal expressions ("last hour", "yesterday", "before the error") to time ranges
- Identifies which primitives to search based on the query intent
- Generates appropriate filter conditions (metadata filters, key prefixes, event types)

### What Qwen3 does NOT do yet

- No result re-ranking (results scored by existing BM25/vector/RRF pipeline)
- No answer synthesis (no generated text in the response)
- No query expansion (no synonym generation or concept broadening)
- No multi-step retrieval (no "search, then search based on those results")

---

## Qwen3 Runtime

Extends the nano inference runtime (v0.7) with decoder-only LLM support.

- **Model**: Qwen3-1.3B, ~1GB RAM (Q4 quantized)
- **KV cache management**: Reuse cache across queries in the same session
- Downloaded on first use, checksum-verified
- If model unavailable, NL search is disabled — keyword and vector search still work

### Runtime options

| Option | Pros | Cons |
|--------|------|------|
| **candle** (Rust-native) | Pure Rust, no C++ deps | Fewer optimized kernels for LLM generation |
| **llama.cpp** (FFI) | Battle-tested quantization, broad hardware support | C++ build dependency |

### Feature flag

```toml
[features]
intelligence-llm = []      # Qwen3 NL search (~1GB additional RAM)
```

---

## Memory footprint

| Component | RAM |
|-----------|-----|
| MiniLM-L6-v2 (from v0.7) | ~80 MB |
| Qwen3-1.3B (Q4) | ~1 GB |
| KV cache (short context) | ~100-200 MB |
| **Total** | **~1.4 GB** |

## Dependencies

- v0.8 (enhanced hybrid search — Qwen3 generates queries that the hybrid pipeline executes)

## Open questions

- How much context to feed Qwen3 — just the query string, or recent search/conversation history?
- What prompt template works best for query decomposition?
- How to handle queries that don't map cleanly to primitive operations?
