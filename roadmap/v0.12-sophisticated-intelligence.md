# v0.12: Sophisticated Intelligence

**Theme**: Fine-tuned models, multi-turn context, and agentic workflows. The intelligence layer matures.

v0.7-v0.10 shipped auto-embedding and natural language search using off-the-shelf models. v0.12 makes the intelligence layer genuinely smart: models fine-tuned for Strata's query patterns, search that remembers conversation history, and proactive data organization.

---

## Fine-Tuned Models

Replace off-the-shelf MiniLM and Qwen3 with Strata-specific variants trained on real query decomposition and embedding tasks.

### MiniLM fine-tuning

- Train on (text, retrieval outcome) pairs from real Strata usage
- Optimize embeddings for the kinds of data agents actually store (tool call logs, conversation fragments, config objects)
- Maintain the same 384-dim output for backward compatibility

### Qwen3 fine-tuning

- Train on (natural language query, optimal sub-query decomposition) pairs
- Improve temporal reasoning accuracy ("last hour" → correct timestamp math)
- Better primitive selection (knows when to search events vs. KV vs. vectors)
- Reduce hallucinated sub-queries

### Model update path

- New model versions downloaded alongside old ones
- A/B comparison: run both models, compare result quality
- Rollback if fine-tuned model underperforms on specific query classes

---

## Multi-Turn Context

Search that understands conversation history. The intelligence layer remembers what was asked and found before.

- **"Find more like the last result"**: Use the previous search results as context for a new query
- **"What changed since I last asked?"**: Compare current state to the state at the time of the last query
- **Pronoun resolution**: "Search for errors in *that* branch" → resolves "that" from conversation history
- **Session-scoped context window**: Recent queries and results are kept in Qwen3's context

### Implementation

- Query history stored in a session-scoped ring buffer (not persisted across restarts)
- Qwen3 receives recent (query, results) pairs as context alongside the new query
- Context window bounded to prevent unbounded memory growth

---

## Agentic Workflows

The intelligence layer proactively organizes, summarizes, and indexes data without being asked.

### Auto-Tagging

- When data is inserted, the intelligence layer infers tags/categories
- Tags stored as metadata on the original entry
- Example: an event with `tool_call` type gets auto-tagged with the tool name, success/failure, and duration bucket

### Background Summarization

- Periodically summarize accumulating data (e.g., "the last 100 events in this branch")
- Summaries stored in `_system_*` collections for fast retrieval
- Keeps the intelligence layer's understanding of the data current

### Smart Indexing

- Intelligence layer identifies frequently queried patterns and precomputes indexes
- Example: if agents frequently search for "recent errors", maintain a materialized view
- Transparent to the user — search just gets faster

---

## Feature flag

```toml
[features]
intelligence-full = ["intelligence-embed", "intelligence-llm"]  # All intelligence features
```

## Dependencies

- v0.10 (advanced NL search — fine-tuning builds on established search pipeline)
- v0.7 (auto-embedding — fine-tuned MiniLM replaces the off-the-shelf model)

## Open questions

- How much training data is needed for meaningful fine-tuning improvements?
- Should multi-turn context persist across sessions (in the event log)?
- How to measure whether fine-tuned models are actually better?
- Resource budget for background agentic workflows — how much CPU/memory can they consume?
