# v0.12.5: Cross-Encoder Re-ranking

**Theme**: Precision boost. After retrieval and fusion, a cross-encoder reranker rescores the top candidates for better ranking quality. No API change — `search()` gets more accurate when a reranker model is configured.

---

## Why re-ranking

RRF fusion is good at recall (finding relevant results) but mediocre at precision (ordering them correctly). BM25 scores are lexical — they can't understand semantic relevance. Vector similarity helps but is coarse at distinguishing near-ties.

A cross-encoder reranker takes (query, document) pairs and produces fine-grained relevance scores. It sees both the query and the full document text together, enabling much deeper relevance assessment than independent BM25/vector scoring.

**Impact from QMD**: Adding re-ranking after RRF fusion improved nDCG@10 measurably — the right result moved from "top 5" to "top 1-2" for natural language queries.

---

## Architecture

### Current flow (v0.12)
```
search("query")
    → [optional] query expansion via LLM
    → multi-pass HybridSearch (BM25 + vector)
    → RRF fusion
    → return top-k results
```

### New flow (v0.12.5)
```
search("query")
    → [optional] query expansion via LLM
    → multi-pass HybridSearch (BM25 + vector)
    → RRF fusion → top-N candidates
    → [optional] cross-encoder rerank top-N
    → position-aware score blending
    → return top-k results
```

### What the reranker does

1. Takes top-N candidates from RRF fusion (default N=20)
2. For each candidate, retrieves the snippet text
3. Sends (query, document_text) pairs to a reranker endpoint
4. Receives relevance scores (0.0–1.0)
5. Blends reranker scores with RRF scores using position-aware weighting
6. Returns re-sorted results

### What the reranker does NOT do

- No answer generation
- No query modification
- No multi-step retrieval
- Only rescores existing candidates — cannot introduce new results

---

## Position-Aware Score Blending

Borrowed from QMD. The key insight: trust retrieval more for high-confidence matches, trust the reranker more for uncertain ones.

```
RRF rank 1-3:   75% RRF score + 25% reranker score
RRF rank 4-10:  60% RRF score + 40% reranker score
RRF rank 11+:   40% RRF score + 60% reranker score
```

**Why**: If BM25 found an exact keyword match at rank #1, the reranker shouldn't demote it. But for lower-ranked results where BM25/vector scores are close, the reranker's semantic understanding breaks ties better.

---

## Reranker API Format

Uses the emerging `/rerank` endpoint standard (supported by Jina, Cohere, vLLM, TEI, and others):

```
POST {endpoint}/rerank
{
    "model": "...",
    "query": "what tools did the agent use?",
    "documents": [
        "The agent invoked search and code generation tools.",
        "Configuration was updated at 3pm.",
        ...
    ],
    "top_n": 20
}

Response:
{
    "results": [
        {"index": 0, "relevance_score": 0.92},
        {"index": 1, "relevance_score": 0.31},
        ...
    ]
}
```

**Why `/rerank` not `/chat/completions`**: Cross-encoder reranking is a dedicated task. The `/rerank` API is purpose-built — it accepts document arrays, returns per-document scores, and is implemented natively by reranker-specific models. Using chat completions would require prompt engineering, output parsing, and wouldn't support batched scoring.

**Compatible providers**: Jina Reranker API, Cohere Rerank, vLLM with `--task rerank`, Hugging Face TEI, Ollama (planned), any server implementing the `/rerank` endpoint.

---

## Configuration

Reuse the existing `configure_model()` pattern. Add optional reranker fields:

```python
db.configure_model(
    endpoint="http://localhost:11434/v1",
    model="qwen3:1.7b",           # expansion model
    reranker_model="jina-reranker-v2-base-multilingual",  # reranker model
)
```

- **`reranker_model` set**: Re-ranking enabled. Uses the same endpoint as expansion (most providers serve both).
- **`reranker_model` not set**: No re-ranking. Search works exactly as v0.12.
- **Separate endpoint**: Optional `reranker_endpoint` for when the reranker runs on a different server.

---

## Graceful degradation

| Failure | Behavior |
|---------|----------|
| No reranker configured | v0.12 behavior (RRF only) |
| Reranker endpoint unreachable | Return RRF results as-is |
| Reranker times out | Return RRF results as-is |
| Reranker returns malformed response | Return RRF results as-is |
| Too few results to rerank (<3) | Skip reranking |

Re-ranking never hard-fails. Worst case is v0.12 quality.

---

## Rerank budget

| Parameter | Default | Rationale |
|-----------|---------|-----------|
| Candidates to rerank | 20 | Balances latency vs. reorder opportunity |
| Minimum results to trigger | 3 | No point reranking 1-2 results |
| Timeout | 3000ms | Reranking should be fast; don't block search |
| Max document length | 2000 chars | Truncate snippets to cap token usage |

---

## Memory footprint

| Component | RAM |
|-----------|-----|
| MiniLM-L6-v2 (from v0.7) | ~80 MB |
| Query expansion model | 0 MB (external API) |
| Reranker model | 0 MB (external API) |
| **Total** | **~80 MB** |

No additional memory. The reranker runs externally like the expansion model.

---

## Dependencies

- v0.12 (query expansion pipeline — reranking builds on the same search flow)
- `ureq` (already a workspace dep — for HTTP calls to reranker endpoint)

## Open questions

- Should reranking apply to strong-signal results too? (Currently strong signal skips both expansion AND reranking — should it skip expansion but still rerank?)
- Optimal N for reranking — 20 is QMD's default, but should this be user-configurable?
- Should we cache reranker scores per (query, doc_ref) pair to avoid re-scoring on repeated queries?
