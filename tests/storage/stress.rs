//! Stress Tests
//!
//! Heavy-workload tests for the storage layer. All marked #[ignore] for opt-in execution.
//! Run with: cargo test --test storage stress -- --ignored

use strata_core::traits::{SnapshotView, Storage};
use strata_core::types::{Key, Namespace};
use strata_core::value::Value;
use strata_core::RunId;
use strata_storage::sharded::ShardedStore;
use std::sync::atomic::{AtomicBool, AtomicU64, Ordering};
use std::sync::Arc;
use std::thread;
use std::time::{Duration, Instant};

fn create_test_key(run_id: RunId, name: &str) -> Key {
    let ns = Namespace::for_run(run_id);
    Key::new_kv(ns, name)
}

/// High-concurrency writers and readers
#[test]
#[ignore]
fn stress_concurrent_writers_readers() {
    let store = Arc::new(ShardedStore::new());
    let run_id = RunId::new();
    let stop = Arc::new(AtomicBool::new(false));
    let writes = Arc::new(AtomicU64::new(0));
    let reads = Arc::new(AtomicU64::new(0));

    // Pre-populate
    for i in 0..100 {
        let key = create_test_key(run_id, &format!("key_{}", i));
        Storage::put(&*store, key, Value::Int(i), None).unwrap();
    }

    // Spawn 4 writers
    let writer_handles: Vec<_> = (0..4)
        .map(|t| {
            let store = Arc::clone(&store);
            let stop = Arc::clone(&stop);
            let writes = Arc::clone(&writes);
            thread::spawn(move || {
                let mut counter = 0i64;
                while !stop.load(Ordering::Relaxed) {
                    for i in 0..100 {
                        let key = create_test_key(run_id, &format!("key_{}", i));
                        let _ = Storage::put(&*store, key, Value::Int(t * 10000 + counter), None);
                        writes.fetch_add(1, Ordering::Relaxed);
                    }
                    counter += 1;
                    if counter > 500 {
                        break;
                    }
                }
            })
        })
        .collect();

    // Spawn 8 readers
    let reader_handles: Vec<_> = (0..8)
        .map(|_| {
            let store = Arc::clone(&store);
            let stop = Arc::clone(&stop);
            let reads = Arc::clone(&reads);
            thread::spawn(move || {
                while !stop.load(Ordering::Relaxed) {
                    for i in 0..100 {
                        let key = create_test_key(run_id, &format!("key_{}", i));
                        let _ = Storage::get(&*store, &key);
                        reads.fetch_add(1, Ordering::Relaxed);
                    }
                    if reads.load(Ordering::Relaxed) > 500_000 {
                        break;
                    }
                }
            })
        })
        .collect();

    // Wait for writers
    for h in writer_handles {
        h.join().unwrap();
    }

    stop.store(true, Ordering::Relaxed);

    // Wait for readers
    for h in reader_handles {
        h.join().unwrap();
    }

    println!(
        "Writes: {}, Reads: {}",
        writes.load(Ordering::Relaxed),
        reads.load(Ordering::Relaxed)
    );
}

/// Rapid snapshot creation
#[test]
#[ignore]
fn stress_rapid_snapshot_creation() {
    let store = Arc::new(ShardedStore::new());
    let run_id = RunId::new();

    // Populate
    for i in 0..1000 {
        let key = create_test_key(run_id, &format!("key_{}", i));
        Storage::put(&*store, key, Value::Int(i), None).unwrap();
    }

    let start = Instant::now();
    let mut snapshots = Vec::new();

    for _ in 0..100_000 {
        snapshots.push(store.snapshot());
    }

    let elapsed = start.elapsed();
    println!(
        "100K snapshots in {:?} ({:.0} snapshots/sec)",
        elapsed,
        100_000.0 / elapsed.as_secs_f64()
    );

    // Verify snapshots work
    for snapshot in snapshots.iter().take(100) {
        let key = create_test_key(run_id, "key_0");
        let result = SnapshotView::get(snapshot, &key).unwrap();
        assert!(result.is_some());
    }
}

/// Deep version chain growth
#[test]
#[ignore]
fn stress_version_chain_growth() {
    let store = ShardedStore::new();
    let run_id = RunId::new();
    let key = create_test_key(run_id, "deep_chain");

    let start = Instant::now();

    // Create 100K versions of the same key
    for i in 0..100_000i64 {
        Storage::put(&store, key.clone(), Value::Int(i), None).unwrap();
    }

    let write_elapsed = start.elapsed();

    // Read latest
    let read_start = Instant::now();
    for _ in 0..10_000 {
        let _ = Storage::get(&store, &key);
    }
    let read_elapsed = read_start.elapsed();

    // Get history
    let history = Storage::get_history(&store, &key, Some(100), None).unwrap();
    assert_eq!(history.len(), 100);

    println!(
        "100K writes: {:?}, 10K reads: {:?}",
        write_elapsed, read_elapsed
    );
}

/// TTL expiration under load
#[test]
#[ignore]
fn stress_ttl_expiration_cleanup() {
    let store = ShardedStore::new();
    let run_id = RunId::new();

    // Insert 10K keys with short TTL
    let ttl = Some(Duration::from_millis(100));
    for i in 0..10_000 {
        let key = create_test_key(run_id, &format!("ttl_key_{}", i));
        Storage::put(&store, key, Value::Int(i), ttl).unwrap();
    }

    // Wait for expiration
    thread::sleep(Duration::from_millis(200));

    // All should be expired
    let mut expired_count = 0;
    for i in 0..10_000 {
        let key = create_test_key(run_id, &format!("ttl_key_{}", i));
        if Storage::get(&store, &key).unwrap().is_none() {
            expired_count += 1;
        }
    }

    println!("Expired: {} / 10000", expired_count);
    assert_eq!(expired_count, 10_000, "All keys should be expired");
}

/// Many runs concurrent access
#[test]
#[ignore]
fn stress_many_runs_concurrent() {
    let store = Arc::new(ShardedStore::new());
    let num_runs = 100;
    let keys_per_run = 100;

    let start = Instant::now();

    let handles: Vec<_> = (0..num_runs)
        .map(|_| {
            let store = Arc::clone(&store);
            thread::spawn(move || {
                let run_id = RunId::new();
                for i in 0..keys_per_run {
                    let key = create_test_key(run_id, &format!("key_{}", i));
                    Storage::put(&*store, key, Value::Int(i), None).unwrap();
                }
                run_id
            })
        })
        .collect();

    let run_ids: Vec<RunId> = handles.into_iter().map(|h| h.join().unwrap()).collect();

    let write_elapsed = start.elapsed();

    // Verify all data
    let verify_start = Instant::now();
    for run_id in run_ids {
        for i in 0..keys_per_run {
            let key = create_test_key(run_id, &format!("key_{}", i));
            let val = Storage::get(&*store, &key).unwrap();
            assert!(val.is_some());
        }
    }
    let verify_elapsed = verify_start.elapsed();

    println!(
        "{} runs x {} keys: write {:?}, verify {:?}",
        num_runs, keys_per_run, write_elapsed, verify_elapsed
    );
}

/// Sustained throughput measurement
#[test]
#[ignore]
fn stress_sustained_throughput() {
    let store = ShardedStore::new();
    let run_id = RunId::new();

    let duration = Duration::from_secs(5);
    let start = Instant::now();
    let mut ops = 0u64;

    while start.elapsed() < duration {
        let key = create_test_key(run_id, &format!("key_{}", ops % 1000));
        Storage::put(&store, key.clone(), Value::Int(ops as i64), None).unwrap();
        let _ = Storage::get(&store, &key);
        ops += 2; // put + get
    }

    let elapsed = start.elapsed();
    println!(
        "Sustained: {} ops in {:?} ({:.0} ops/sec)",
        ops,
        elapsed,
        ops as f64 / elapsed.as_secs_f64()
    );
}
